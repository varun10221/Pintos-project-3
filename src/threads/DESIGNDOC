      +--------------------+
      |        CS 140      |
      | PROJECT 1: THREADS |
      |   DESIGN DOCUMENT  |
      +--------------------+
           
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Jamie Davis <davisjam@vt.edu>
Mai Dahshan <mdahshan@vt.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

           ALARM CLOCK
           ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

timer_sleep() appends the running thread to the sleeping_list.
This update to the sleeping_list is kept atomic through the use of a lock.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

The situation here is that a thread is running timer_sleep and a timer
interrupt occurs. 

CASE: The timer interrupt handler traverses and potentially removes elements from the sleeping_list.
It does not hold a lock. I don't see a good way around this race condition besides disabling interrupts
in timer_sleep while we modify sleeping_list.

CASE: The timer interrupt handler just Ups a semaphore. This (eventually) wakes waker_thread, who
waits for the lock on the sleeping_list before modifying it. No race condition since we revert to
well-defined lock semantics.

HOWEVER: if the timer interrupt handler interrupts a call to timer_sleep while we are computing
when to wake us up, and it uses X ticks, then we potentially sleep X + ticks_to_sleep ticks.
If X >> ticks_to_sleep, this could be bad.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

For Monday 31 Aug:
Jamie's proposed design

  As provided, timer_sleep calls thread_yield until ticks <= timer_elapsed.
  thread_yield puts the caller at the end of the ready list, leaves its state as THREAD_READY, and then schedules a thread to run
    (could be the caller). This can result in busy looping, using up CPU cycles scheduling and unscheduling the sleeping thread
    until enough ticks have passed. For short sleeps this is OK, but for long sleeps this wastes a lot of CPU.

  It would be preferable if the sleeping thread were not considered for scheduling until its requested number of ticks have passed.
  There is no requirement that the thread be woken up after exactly x ticks (unless the system is otherwise idle).

  Proposal 1: Complete accuracy, at the expense (perhaps) of system responsiveness
  
  - Introduce a new list of sleeping threads in thread.c: static struct list sleeping_list
  - Add a wake_me_at field to struct thread, used to track the minimum value of ticks in 'ticks' before which we can be woken (this could also be done with a struct tracking a thread and a wake_me_at field, but this seems like a needless use of memory)
  - When a thread calls timer_sleep, it adds itself to this list and yields
  - In the timer interrupt handler, traverse the sleeping_list and add to ready_list any threads list that have finished their sleep

  Problem: If the system has a lot of threads that call timer_sleep, traversing the sleeping_list to find threads that should be woken can be expensive.
    The interrupt handler must run as fast as possible to improve system responsiveness. We don't want to spend ticks iterating over a list.

  Variation: keep the sleeping_list sorted by soonest-to-wake, minimizing the time spent traversing it in the timer interrupt handler. This makes insertion more expensive, which in turn makes the timer interrupt handler take longer.
    
  Proposal 2: System responsiveness, at the expense of timer accuracy

  (The first three are the same as Proposal 1)
  - Introduce a new list of sleeping threads in thread.c: static struct list sleeping_list
  - Add a wake_me_at field to struct thread, used to track the minimum value of ticks in 'ticks' before which we can be woken
  Differences:
  - Introduce another "system" thread (joining the elite club of idle_thread and initial_thread). Call it waker_thread.
     waker_thread is always on the ready list and when scheduled, it:
      - iterates over the sleeping_list
      - adds any ready-to-wake threads to the ready_list
      - yields

  Here we're still burning CPU cycles busy-waiting, but in a high-sleeper-count scenario only *one* thread (the waker_thread) is being scheduled to burn cycles instead of multiple threads. In a low-sleeper-count scenario this is roughly the same as what we have now anyway.

  Variation 1: Instead of introducing a waker_thread, have the idle_thread check the sleeper_list instead. It's not doing anything else anyway. However, this could starve sleepers. The idle_thread only runs when the ready_list is empty, so if there are always ready threads, idle would never run and thus sleepers would never be woken. Failure!

  Variation 2: Have waker_thread wait on a semaphore (monitor?) and have the timer interrupt "Up" the semaphore. This does not require busy waiting. waker_thread is therefore only ever scheduled to run by the timer interrupt, instead of at the same rate as every other thread in the ready list.

  Variation 3: Just stick sleeping threads in the ready_list, mark them as SLEEPING, and don't schedule them until enough time has passed. This has minimal complexity but makes the scheduling activity pricey! I think the list traversal could become unpleasantly expensive.

  - Note from Mai: The current implementation could wake a thread sleeping for 10 ticks BEFORE(?) a thread sleeping for 12 ticks (well not quite because it's FIFO but they could be woken at the same time. Strange.)

  Files to modify:
    - thread.h: thread_status
    - thread.h: struct thread

Mai's proposed design
  TODO

       PRIORITY SCHEDULING
       ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

For Monday 31 Aug:
Jamie's proposed design

Design components:

1. Implement priority scheduling in Pintos. When a thread is added to the ready list that has a higher priority than the currently running thread, the current thread should immediately yield the processor to the new thread

  - Unclear to me how to get the RUNNING (current) thread to decide to yield the processor all by itself. 

    Seems like immediately following the addition of threads to ready_list, we should make schedule() go off.

    When are threads added to the ready list?
      - thread_yield adds to the ready list (and yields, which calls schedule(), so no need to preempt)
      - waker adds to the ready list, and since it is running on its own it is allowed to preempt the running thread
        > However, waker then yields, which calls schedule(), so we self-preempt
      - thread_unblock adds the current thread to the ready list. However, thread_unblock is specifically documented *not*
        to preempt the running thread, so that's not an option.

    Let's ignore thread_yield and waker, since they seem irrelevant. Focus on thread_unblock.
    Who calls thread_unblock? 
      According to cscope:
        - thread_create is the source of threads, and it calls thread_unblock 
        - sema_up calls thread_unblock

        If we call thread_yield() immediately after finishing the critical section containing thread_unblock in both
          of these functions, we ensure that the highest-priority thread (which might be the newly-unblocked thread) 
          will be run.

  - schedule() calls next_thread_to_run(), which is really in charge of identifying the next thread to run 

    At the moment, this pops ready_list (so first-come first-served) or returns idle_thread

    Proposed enhancements:
      > Iterate over the entire ready_list, identify the highest-priority thread closest to the front of the list, and run it.
        This has the noteworthy drawback of increasing the time required to schedule a thread, potentially significantly.
        Scheduling cost is O(N) where N is the number of processes running.
      > Introduce (1 + PRI_MAX - PRI_MIN) ready_lists. Iterate from high priority to low priority. We remove the first element from the
        first non-empty list we find. Scheduling cost is now O(1), since we scan a constant number of lists and identifying the thread
        to run is done with a FIFO.

        This costs 32bytes-per-list * 64 lists == idk bytes. Relatively small overhead considering the size and quantity of TCBs we'll be allocating.

      > Introduce the same number of sleeping_lists, in the same fashion. This allows waker to wake threads in the order of appropriate priority.


2. Similarly, when threads are waiting for a lock, semaphore, or condition variable, the highest priority waiting thread should be awakened first. 

  > Introduce a function called by next_thread_to_run that looks through an array or list of lists for the highest-priority thread.
    Then call the same function in next_thread_to_run and a new function next_thread_to_wake.
    There is no need to use this function in the waker thread because he is going to iterate over all priorities every time he runs.

3. A thread may raise or lower its own priority at any time, but lowering its priority such that it no longer has the highest priority must cause it to immediately yield the CPU.

   If a thread raises its priority, well, it is already running, so no need to do anything
   If a thread lowers its priority, it should thread_yield() and give the scheduler a chance to pick a higher-priority thread

4. One issue with priority scheduling is "priority inversion". Consider high, medium, and low priority threads H, M, and L, respectively. If H needs to wait for L (for instance, for a lock held by L), and M is on the ready list, then H will never get the CPU because the low priority thread will not get any CPU time. A partial fix for this problem is for H to "donate" its priority to L while L is holding the lock, then recall the donation once L releases (and thus H acquires) the lock. 
  Implement priority donation. You will need to account for all different situations in which priority donation is required. Be sure to handle multiple donations, in which multiple priorities are donated to a single thread. You must also handle nested donation: if H is waiting on a lock that M holds and M is waiting on a lock that L holds, then both M and L should be boosted to H's priority. If necessary, you may impose a reasonable limit on depth of nested priority donation, such as 8 levels.

You must implement priority donation for locks. You need not implement priority donation for the other Pintos synchronization constructs. You do need to implement priority scheduling in all cases.

-----

  Cases where priority donation is required:
    A thread should only donate priority when it is waiting on another thread. At the moment, this only occurs when waiting on a synchronization variable.
    Later this could include waiting for use of hardware (e.g. a printer) or other system resources.
    However, in most (all?) cases I would hope that such access is controlled by use of synchronization primitives, such that
    supporting it in the context of synchronization primitives gives us priority donation in a wide variety of cases.

  Concepts:
    > Threads maintain a list of the lock resources they hold.
    > Threads track both a priority and an effective priority (same idea as uid/effective uid)
    > Lock resources know their holder.
    > When a thread blocks on a lock resource, it determines the current holder of the resource and promotes it if the blocking thread has a higher priority.
    > When a thread releases a lock resource, it has to correct its priority.

  Design:

  Struct changes:
    Add to the TCB a list of currently-held resources.
    Add to the TCB the resource for which we are waiting (allows nested donation).

  1. When a thread goes to lock a resource and has to block: 
  - in its TCB it sets the resource it is waiting for
  - it may update the current holder's priority

  Code from sema_down:
  
    old_level = intr_disable ();
    while (sema->value == 0) 
      {
        list_push_back (&sema->waiters, &thread_current ()->elem);
        /* NEW CODE */
        update_waiting_resource ();
        scheduler_donate_priority (holder_of_waiting_resource);
        /* END OF NEW CODE */
        thread_block ();
      }
    intr_set_level (old_level);

  TODO We could also disable interrupts in lock/unlock itself (including the call to sema_down?) 
    and put this logic into the lock instead of the semaphore?

  (We can keep the sema->waiters as a simple linked list. If this list is extremely long I think that implies
    a design flaw or a very overloaded system anyway.)

  update_waiting_resource identifies the current thread and sets the resource-for-which-we-are-waiting field.

  scheduler_donate_priority will move the holder to the run queue of the appropriate priority, based on the priority
    of the current thread and the effective priority of the holder.
    - We can assert that the thread is found in the run queue of its effective priority
    - Finding the holder thread in its run list may be expensive.
    - If the holder itself is waiting, recurse on the holder, thus allowing nested donation. 

    ** Either cap max donation at, say, 8, or remember all of the threads we encounter in order
        to detect a deadlock (which would otherwise cause us to go into infinite recursion).
        Note that we should NOT make this an assert: user-level processes are allowed to deadlock if they want to.
        We could, however, kill the threads involved in the deadlock (if we had a function for that).
        At the moment a thread has to voluntarily exit.

  Note that a semaphore does not know the holder, since there is no holder for a semaphore.
    sema_down will be modified to accept an optional *resource (defaulting to NULL but provided in the call from lock_acquire) for this purpose.
    Other synchronization primitives build on semaphores may also track their holder, so this allows some generalization. 

  Note that this scheme causes a given holder to track only its *highest* priority waiter, which brings us to...

  2. After a thread unlocks a resource, any increase in its priority due to other threads 
  waiting on the resource should be revoked. However, it may still be donated higher priority
  by threads waiting on OTHER resources that it holds.

  Consequently, after unlocking a resource a thread should call scheduler_revoke_priority,
    which will determine the appropriate priority for this thread based on the current priorities of the waiters of the locks that it holds.
    Since scheduler_donate_priority is recursive, we don't need to worry about recursing "backwards" here: the highest effective
    priority of its waiters will be the maximum priority the thread can take on.
          
  Both of these operations require a lock on the ready_list information.
    Before blocking on a lock (in sema_down) we have disabled interrupts, effectively giving us a lock on every global data structure.
    After releasing a lock (in sema_up), we have disabled interrupts. Same idea. 

      If thread A releases the lock *without* lowering its priority as needed, it risks cascading bad priority values to other threads
      due to an interrupt and another thread blocking on a resource that thread A holds.

  Drawbacks:
    scheduler_donate_priority and scheduler_revoke_priority will require locating the thread in the list for its current priority level.
    This is somewhat expensive. Doing this without disabling interrupts seems awkward in a system with a high process count. 
    Doing it with interrupts enabled, though, makes it possible for the current holder of the lock to change out from under us.
    If we use a stale value for holder, we risk raising the priority of the holder PERMANENTLY, with nasty repercussions for 
    threads that actually need high priority.

    (because the old holder has released the lock already, and we donate to him assuming he will yield his priority later
      after releasing the lock)

  Q: Does this design handle nested donation?
  A: Yes.

  Promotion:
    L runs, acquires resource 1, and then yields.
    M runs, acquires resource 2, then tries to acquire resource 1. It blocks waiting for L. L's priority is raised to M.
    H runs, acquires resource 3, then tries to acquire resource 2. It blocks waiting for M. M's priority is raised to H.
      We then recurse on the thread for which M is waiting (if there is one). L's priority is raised to H.
      Promotion has been propagated appropriately.

  Demotion:
    Now L is scheduled. L releases the lock and no longer cares about any waiters on that lock. L examines the priorities the waiters on the resources it still holds,
      and modifies its priority to max( L's original priority, highest priority among waiters ).
      L can assert here that it finds no priority larger than the one it is giving up, 
      since a thread adding itself to waiters and then calling scheduler_donate_priority is done atomically.

    Proposal:
      Threads track the holder of the resource for which they are waiting, updating it before thread_block in the synchronization code.
      scheduler_donate_priority can work recursively on this field.

        ADVANCED SCHEDULER
        ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0
 4
 8
12
16
20
24
28
32
36

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

         SURVEY QUESTIONS
         ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

