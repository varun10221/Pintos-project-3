      +--------------------+
      |        CS 140      |
      | PROJECT 1: THREADS |
      |   DESIGN DOCUMENT  |
      +--------------------+
           
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Jamie Davis <davisjam@vt.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

No preliminary comments.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

No other sources were consulted.

           ALARM CLOCK
           ===========

============================
TODO REWRITE THE ALARM CLOCK STUFF
============================


---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

timer_sleep() appends the running thread to the sleeping_queue.
This update to the sleeping_queue is kept atomic through the use of a lock.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

The situation here is that a thread is running timer_sleep and a timer
interrupt occurs. 

CASE: The timer interrupt handler traverses and potentially removes elements from the sleeping_queue.
It does not hold a lock. I don't see a good way around this race condition besides disabling interrupts
in timer_sleep while we modify sleeping_queue.

CASE: The timer interrupt handler just Ups a semaphore. This (eventually) wakes waker_thread, who
waits for the lock on the sleeping_queue before modifying it. No race condition since we revert to
well-defined lock semantics.

HOWEVER: if the timer interrupt handler interrupts a call to timer_sleep while we are computing
when to wake us up, and it uses X ticks, then we potentially sleep X + ticks_to_sleep ticks.
If X >> ticks_to_sleep, this could be bad.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

For Monday 31 Aug:
Jamie's proposed design

  As provided, timer_sleep calls thread_yield until ticks <= timer_elapsed.
  thread_yield puts the caller at the end of the ready list, leaves its state as THREAD_READY, and then schedules a thread to run
    (could be the caller). This can result in busy looping, using up CPU cycles scheduling and unscheduling the sleeping thread
    until enough ticks have passed. For short sleeps this is OK, but for long sleeps this wastes a lot of CPU.

  It would be preferable if the sleeping thread were not considered for scheduling until its requested number of ticks have passed.
  There is no requirement that the thread be woken up after exactly x ticks (unless the system is otherwise idle).

  Proposal 1: Complete accuracy, at the expense (perhaps) of system responsiveness
  
  - Introduce a new list of sleeping threads in thread.c: static struct list sleeping_queue
  - Add a wake_me_at field to struct thread, used to track the minimum value of ticks in 'ticks' before which we can be woken (this could also be done with a struct tracking a thread and a wake_me_at field, but this seems like a needless use of memory)
  - When a thread calls timer_sleep, it adds itself to this list and yields
  - In the timer interrupt handler, traverse the sleeping_queue and add to ready_queue any threads list that have finished their sleep

  Problem: If the system has a lot of threads that call timer_sleep, traversing the sleeping_queue to find threads that should be woken can be expensive.
    The interrupt handler must run as fast as possible to improve system responsiveness. We don't want to spend ticks iterating over a list.

  Variation: keep the sleeping_queue sorted by soonest-to-wake, minimizing the time spent traversing it in the timer interrupt handler. This makes insertion more expensive, which in turn makes the timer interrupt handler take longer.
    
  Proposal 2: System responsiveness, at the expense of timer accuracy

  (The first three are the same as Proposal 1)
  - Introduce a new list of sleeping threads in thread.c: static struct list sleeping_queue
  - Add a wake_me_at field to struct thread, used to track the minimum value of ticks in 'ticks' before which we can be woken
  Differences:
  - Introduce another "system" thread (joining the elite club of idle_thread and initial_thread). Call it waker_thread.
     waker_thread is always on the ready list and when scheduled, it:
      - iterates over the sleeping_queue
      - adds any ready-to-wake threads to the ready_queue
      - yields

  Here we're still burning CPU cycles busy-waiting, but in a high-sleeper-count scenario only *one* thread (the waker_thread) is being scheduled to burn cycles instead of multiple threads. In a low-sleeper-count scenario this is roughly the same as what we have now anyway.

  Variation 1: Instead of introducing a waker_thread, have the idle_thread check the sleeper_list instead. It's not doing anything else anyway. However, this could starve sleepers. The idle_thread only runs when the ready_queue is empty, so if there are always ready threads, idle would never run and thus sleepers would never be woken. Failure!

  Variation 2: Have waker_thread wait on a semaphore (monitor?) and have the timer interrupt "Up" the semaphore. This does not require busy waiting. waker_thread is therefore only ever scheduled to run by the timer interrupt, instead of at the same rate as every other thread in the ready list.

  Variation 3: Just stick sleeping threads in the ready_queue, mark them as SLEEPING, and don't schedule them until enough time has passed. This has minimal complexity but makes the scheduling activity pricey! I think the list traversal could become unpleasantly expensive.

  Files to modify:
    - thread.h: thread_status
    - thread.h: struct thread


Design update during priority queue testing:
  Using a raw priority_queue for the sleeping_queue seems to be too expensive to scan
  (at least to pass the alarm-multiple test).

  This may be due to the waste of timer cycles running the waker_thread unnecessarily.
  At the moment we run it any time there is a sleeper and timer_interrupt goes off.

Proposal 1:
  To reduce waste in running waker, we choose to keep the sleeping_queue sorted by wake time.
  T1 comes before T2 if
    T1.wake_me_at < T2.wake_me_at
  This causes us to spend extra CPU cycles inserting, but makes it much cheaper to determine whether
  or not there is a sleeping thread to wake. 
    > Compute MIN(wake_me_at of the first thread in each list of the priority queue)

  If only one priority level is dominant, the use of a 64-list queue is a mistake, since we
  don't derive much benefit from splitting it up and we have to spend awhile iterating
  through the 64 lists each time.

  Competing with Proposal 2, can we track the next time that ANY thread needs to be woken? 
  This would allow timer_interrupt to decide very quickly whether or not there is anything 
    for waker to do.

  - Add an int64_t field to the priority_queue struct, for general use
  - Any time a thread calls push_sleeping_queue, we update the field
  - Any time waker actually runs, it knows there is someone to wake up. It wakes the first elements
    of each list and peeks at the next element of each (if there is someone). It then computes
    MIN(peek value) and updates the field for timer_interrupt. Since it is doing so
    atomically w.r.t. sleeping_queue (it holds a lock), we have the invariant "the next time to wake
    field of sleeping_queue will always be correct" 

  Note that we cannot track the priority queue to start in, since waker may not run in the same tick
    that timer_interrupt goes off. Consequently there may be other threads at a higher priority
    level that should be woken first, even if they were supposed to be woken at a later tick
    than the min in the sleeping_queue.

Proposal 2:
  We could revert to using a list rather than a priority_queue for the sleeping_queue.
  We would keep it sorted by both priority AND wake_time, giving precedence to wake_time.
  T1 comes before T2 if
    T1.wake_me_at < T2.wake_me_at
      or
    T1.wake_me_at == T2.wake_me_at && T1.effective_priority < T2.effective_priority
       
  This would make it extremely cheap to determine whether or not there was someone to wake.
    > Check the wake_me_at time of the first element in the queue
  However, insertion would become expensive if there is a high thread count and there are multiple
  priority levels with high thread counts.

  If only one priority level is dominant, the cost of insertion would be comparable to the 
  prioritized list.

Both proposals rely on the fact that a thread cannot change its wake_me_at time until it is woken up,
  since re-sorting a list would be expensive.

-------------------

Final alarm clock design:
  After experiencing "surprising" thread behavior due to the interplay of the scheduler, the waker thread, and other threads
  doing sleeping, and after consultation with Dr. Back, I decided to change the design to look like the original Proposal 1,
  with a few tweaks.

1. Put the waker logic directly into timer_interrupt. Signaling the waker thread puts us at the mercy of the 
OS scheduler, an undesirable situation given the critical nature of the alarm clock.
2. Keep the sleeping_queue, but make it directly available to timer.h (no longer static). 
3. Keep this sleeping_queue a priority queue (for priority waking), and keep each list sorted by wake time.
  This allows us to quickly find and wake the relevant threads.
  
       PRIORITY SCHEDULING
       ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread:
    priority base_priority;             /* Base priority. */
    priority effective_priority;        /* Effective priority. */
    struct priority_queue_elem elem;    /* Priority queue element. */
    /* Tracks locks held by this thread. */
    struct list lock_list; /* Locks we hold. */
    /* Tracks the lock for which this thread is waiting, enabling
       nested priority donation. */
    struct lock *pending_lock; /* Lock we want. */

struct semaphore_elem:
    This new field allows us to identify the "largest" element
    in a list of semaphore_elem's. It allows priority wakeup.
    int64_t *sort_val;                   /* Value on which to sort. */

New files:
  priority_queue.h, priority_queue.c

  Define interface to and implementation of data structure 'struct priority_queue'

/* Mask what a priority is. */
typedef int64_t priority;

/* Like a list_elem, but for a priority queue. */
struct priority_queue_elem
  {
    struct list_elem elem; /* List element. */
    priority *p;           /* Priority of the element. */
    int64_t *sort_val;     /* Sort by this value if desired. */
  };

/* Priority queue, an ADT modeled after the list implementation. */
#define PRI_QUEUE_NLISTS (1 + PRI_MAX - PRI_MIN)
struct priority_queue
  {
    struct list queue[PRI_QUEUE_NLISTS];
    size_t size; /* Total number of elements in all lists. */
    /* Useful for the sleeping_queue. Could be handy in other places.
       In the sleeping list we track the minimum wake_me_at value. */  
    int64_t val; 
  };

These two were changed to be 'struct priority_queues' rather than 'struct lists'. 
This allows faster look-up time in the event of thousands or more threads.
I changed the names, too. They are declared as static structs invisible outside
of thread.c, so there is no risk of breaking existing applications.

  static struct priority_queue ready_queue;
  static struct priority_queue sleeping_queue;

synch.h:
  The waiters lists of condition variables will remain unsorted lists.
  The list elements (priority_queue_elem or semaphore_elem) both have
  a sort_val which is set to the address of the effective priority
  of the relevant thread, allowing us to determine the
  highest priority waiter just-in-time.

  Using lists rather than priority queues keeps the size of a 'struct lock' relatively small.
  Applications (e.g. priority-donate-chain.c) might declare enough locks
  to clobber the stack if priority queues were used instead of lists 
  in this setting. I started out using priority queues, and switched back
  when I ran priority-donate-chain and encountered stack corruption.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

TODO

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

The waiters list for each synchronization primitive has elements that
know their priority. With appropriately defined list_less_func's
(one for sema's and one for cond's), list_max can be used to find
the highest-priority waiter.

This takes O(N) time but we expect that synchronization primitives
will not have a large number of waiters at any given time.

For the sleeping_queue and the ready_queue, priority_queue_pop_front
finds the highest-priority waiter in O(1) time. These lists are
expected to be larger, and consequently O(1) time is a design requirement.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation. How is nested donation handled?

A thread determines that the lock is already held.
It determines whether or not the holder would accept a donation of
priority (i.e. does the thread have higher priority than the holder?).

If the holder would accept the donation:
  Since it is not running, the holder must be in either "ready" or "blocked" state.
    In "ready" state it is on the ready_queue.
    In "blocked" state it is on the sleeping_queue (if sleeping)
      or on the list of waiters associated with a synchronization primitive
      (sema, lock, or cond).
  The thread determines the state and location of the holder and promotes
    him (raises his effective priority)
    If on the ready queue or the sleeping queue, the holder must be relocated within
    the queue.
    If on the waiting list of a synchronization primitive, just changing the 
    effective priority is sufficient: the highest priority waiter is calculated "just in time" 
    in the synchronization primitives.

  If the holder is blocked waiting on a lock (as determined by the 
    'struct lock *lock_we_want' field in a 'struct thread'), the caller
    looks up the holder and repeats the operation up to MAX_DONATION_DEPTH times. 

  If the holder is blocked on a synchronization primitive other than a lock, there is no
    particular thread responsible for signaling the primitive. In this case 
    donation ceases, as there is no one to donate priority to.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

A thread releases a lock. If there are waiters, the thread
wakes the highest priority one using list_max on the list of waiters.

The thread holding the lock might have had priority donated to it.
Due to priority donation, we know that a thread cannot have a lower
priority than the thread it just woke. If the thread it woke has an equal 
priority, it may have had its current priority donated to it DUE TO OWNERSHIP
OF THIS LOCK. However, if it holds multiple locks then its current priority
could be due to a thread waiting on another lock that it holds.
Consequently, the thread checks through the locks it holds for the highest
waiter on each. 
  If it finds no waiter with its own priority, it knows that
the priority donated to it came from the thread it just woke.
The thread releasing the lock then "returns" its priority by reducing its effective priority
to the max of (its base priority, highest of those threads waiting for the locks it holds).

If a thread reduces its priority as a result of this calculation, the thread
releasing the lock yields to allow the higher-priority thread to run.

This gives precedence to the running thread in the event that it holds multiple locks
and that two or more locks are tied for "highest priority waiter".

Suppose threads A, B, and C, and locks L1, L2.
Thread    priority
  A          1
  B, C       2
A locks L1 and L2.
B locks L1, and later on C locks L2.
B and C both offer priority to A.

Now A runs, releases L1, and sees that B (the thread it just woke) also has priority 2.
A may or may not need to yield to him. A runs thread_return_priority and sees that C is 
waiting for L2 with priority 2, so A retains its "promotion" and does not yield.

We justify this by noting that if the running thread holds two locks, it might be holding more.
As a general rule, we would like it to release more locks to give other threads a chance to run.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

Suppose thread A (priority 2) holds a lock.
Thread B (priority 2) wants the lock.
Suppose thread_set_priority is a 2-step operation: 
  - change the base priority 
  - update effective priority based on the locks I hold

Suppose thread_donate_priority checks the effective priority but not the
  base priority.

Suppose thread_set_priority is not done atomically, so that a thread can
  be interrupted in between changing the base and changing the effective
  priority.

Thread A calls thread_set_priority(1), lowers its base priority, and is then interrupted.
  (base 1 effective 2).

Thread B calls lock_acquire, sees that thread A holds it, and "offers" its priority
to Thread A. Thread B sees that thread A has effective priority 2 and so does not
modify Thread A's effective priority. Thread B blocks and the scheduler runs.

*The scheduler runs and a thread with priority 1 (not A) is run.
  This is in defiance of priority scheduling: after Thread B blocks,
  Thread A should have priority 2 and should have been scheduled.

Thread A is now scheduled and updates its effective priority to 2 because B
is waiting for it.

The role of the scheduler in this race condition means that the only 
useful form of synchronization is to disable interrupts during thread_set_priority.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

As noted above, I considered replacing the waiters lists of synchronization
primitives with priority_queues. 

Changing a thread's priority during priority donation might require moving it within
its list. This operation would be trivial when using a priority queue.
However, a priority_queue is too large and can clobber the stack.
In addition, I consider it unlikely that the list of waiters for a given lock
would ever grow long enough for element relocation to have a meaningful performance impact.

Replacing the ready_queue and sleeping_queue with priority queues, however, seemed
like a logical step. These structures can reasonably be expected to hold thousands of elements.

Changing an element's priority in a 64-way array of lists consists of a list_remove operation (changes a few pointers)
followed by a list_push_back (changes a few more pointers).
To do so in a single list would be a O(n) operation.

-------

Design components:

1. Implement priority scheduling in Pintos. When a thread is added to the ready list that has a higher priority than the currently running thread, the current thread should immediately yield the processor to the new thread

  - Unclear to me how to get the RUNNING (current) thread to decide to yield the processor all by itself. 

    Seems like immediately following the addition of threads to ready_queue, we should make schedule() go off.

    When are threads added to the ready list?
      - thread_yield adds to the ready list (and yields, which calls schedule(), so no need to preempt)
      - waker adds to the ready list, and since it is running on its own it is allowed to preempt the running thread
        > However, waker then yields, which calls schedule(), so we self-preempt
      - thread_unblock adds the current thread to the ready list. However, thread_unblock is specifically documented *not*
        to preempt the running thread, so that's not an option.

    Let's ignore thread_yield and waker, since they seem irrelevant. Focus on thread_unblock.
    Who calls thread_unblock? 
      According to cscope:
        - thread_create is the source of threads, and it calls thread_unblock 
        - sema_up calls thread_unblock

        If we call thread_yield() immediately after finishing the critical section containing thread_unblock in both
          of these functions, we ensure that the highest-priority thread (which might be the newly-unblocked thread) 
          will be run.

  - schedule() calls next_thread_to_run(), which is really in charge of identifying the next thread to run 

    At the moment, this pops ready_queue (so first-come first-served) or returns idle_thread

    Proposed enhancements:
      > Iterate over the entire ready_queue, identify the highest-priority thread closest to the front of the list, and run it.
        This has the noteworthy drawback of increasing the time required to schedule a thread, potentially significantly.
        Scheduling cost is O(N) where N is the number of processes running.
      > Introduce (1 + PRI_MAX - PRI_MIN) ready_queues. Iterate from high priority to low priority. We remove the first element from the
        first non-empty list we find. Scheduling cost is now O(1), since we scan a constant number of lists and identifying the thread
        to run is done with a FIFO.

        This costs 32bytes-per-list * 64 lists == idk bytes. Relatively small overhead considering the size and quantity of TCBs we'll be allocating.

      ** Could consider defining an "abstract type" for this 64-element array with functions to insert, search, remove
      This would let us change the implementation later as desired, and would probably be cleaner than hardcoding
      the 64-element array everywhere.

      > Introduce the same number of sleeping_queues, in the same fashion. This allows waker to wake threads in the order of appropriate priority.


2. Similarly, when threads are waiting for a lock, semaphore, or condition variable, the highest priority waiting thread should be awakened first. 

  > Introduce a function called by next_thread_to_run that looks through an array or list of lists for the highest-priority thread.
    Then call the same function in next_thread_to_run and a new function next_thread_to_wake.
    There is no need to use this function in the waker thread because he is going to iterate over all priorities every time he runs.

  Note that we may actually need two functions here. Not anticipating really long lists of waiters on a resource,
  so we can just leave a linked list for the waiters and find the longest-waiting-highest-priority guy on it.
  (search back to front and track the highest priority found and a pointer to that element)

  This should simplify implementation a bit, and save space in the synchronization primitive.

3. A thread may raise or lower its own priority at any time, but lowering its priority such that it no longer has the highest priority must cause it to immediately yield the CPU.

   If a thread raises its priority, well, it is already running, so no need to do anything
   If a thread lowers its priority, it should thread_yield() and give the scheduler a chance to pick a higher-priority thread

4. One issue with priority scheduling is "priority inversion". Consider high, medium, and low priority threads H, M, and L, respectively. If H needs to wait for L (for instance, for a lock held by L), and M is on the ready list, then H will never get the CPU because the low priority thread will not get any CPU time. A partial fix for this problem is for H to "donate" its priority to L while L is holding the lock, then recall the donation once L releases (and thus H acquires) the lock. 
  Implement priority donation. You will need to account for all different situations in which priority donation is required. Be sure to handle multiple donations, in which multiple priorities are donated to a single thread. You must also handle nested donation: if H is waiting on a lock that M holds and M is waiting on a lock that L holds, then both M and L should be boosted to H's priority. If necessary, you may impose a reasonable limit on depth of nested priority donation, such as 8 levels.

You must implement priority donation for locks. You need not implement priority donation for the other Pintos synchronization constructs. You do need to implement priority scheduling in all cases.

-----

  Cases where priority donation is required:
    A thread should only donate priority when it is waiting on another thread. At the moment, this only occurs when waiting on a synchronization variable.
    Later this could include waiting for use of hardware (e.g. a printer) or other system resources.
    However, in most (all?) cases I would hope that such access is controlled by use of synchronization primitives, such that
    supporting it in the context of synchronization primitives gives us priority donation in a wide variety of cases.

  Concepts:
DONE    > Threads maintain a list of the (lock) resources they hold
DONE    > Threads track both a priority and an effective priority (same idea as uid/effective uid)
DONE    > Lock resources know their holder.
DONE    > When a thread blocks on a lock resource, it determines the current holder of the resource and promotes it if the blocking thread has a higher priority.
DONE    > When a thread releases a lock resource, it has to correct its priority.

  Design:

  Alternative design for consideration:
    Track a global list of locks and identify the holders that way.

  Struct changes:
    Add to the TCB a list of currently-held resources.
    Add to the TCB the resource for which we are waiting (allows nested donation).

  1. When a thread goes to lock a resource and has to block: 
  - in its TCB it sets the resource it is waiting for
  - it may update the current holder's priority

  Code from sema_down:
  
    old_level = intr_disable ();
    while (sema->value == 0) 
      {
        list_push_back (&sema->waiters, &thread_current ()->elem);
        /* NEW CODE */
        update_waiting_resource ();
        scheduler_donate_priority (holder_of_waiting_resource);
        /* END OF NEW CODE */
        thread_block ();
      }
    intr_set_level (old_level);

  TODO We could also disable interrupts in lock/unlock itself (including the call to sema_down?) 
    and put this logic into the lock instead of the semaphore?

  (We can keep the sema->waiters as a simple linked list. If this list is extremely long I think that implies
    a design flaw or a very overloaded system anyway.)

  update_waiting_resource identifies the current thread and sets the resource-for-which-we-are-waiting field.

  scheduler_donate_priority will move the holder to the run queue of the appropriate priority, based on the priority
    of the current thread and the effective priority of the holder.
    - We can assert that the thread is found in the run queue of its effective priority
    - Finding the holder thread in its run list may be expensive.
    - If the holder itself is waiting, recurse on the holder, thus allowing nested donation. 

    ** Either cap max donation at, say, 8, or remember all of the threads we encounter in order
        to detect a deadlock (which would otherwise cause us to go into infinite recursion).
        Note that we should NOT make this an assert: user-level processes are allowed to deadlock if they want to.
        We could, however, kill the threads involved in the deadlock (if we had a function for that).
        At the moment a thread has to voluntarily exit.

  Note that a semaphore does not know the holder, since there is no holder for a semaphore.
    sema_down will be modified to accept an optional *resource (defaulting to NULL but provided in the call from lock_acquire) for this purpose.
    Other synchronization primitives build on semaphores may also track their holder, so this allows some generalization. 

  Note that this scheme causes a given holder to track only its *highest* priority waiter, which brings us to...

  2. After a thread unlocks a resource, any increase in its priority due to other threads 
  waiting on the resource should be revoked. However, it may still be donated higher priority
  by threads waiting on OTHER resources that it holds.

  Consequently, after unlocking a resource a thread should call scheduler_revoke_priority,
    which will determine the appropriate priority for this thread based on the current priorities of the waiters of the locks that it holds.
    Since scheduler_donate_priority is recursive, we don't need to worry about recursing "backwards" here: the highest effective
    priority of its waiters will be the maximum priority the thread can take on.
          
  Both of these operations require a lock on the ready_queue information.
    Before blocking on a lock (in sema_down) we have disabled interrupts, effectively giving us a lock on every global data structure.
    After releasing a lock (in sema_up), we have disabled interrupts. Same idea. 

      If thread A releases the lock *without* lowering its priority as needed, it risks cascading bad priority values to other threads
      due to an interrupt and another thread blocking on a resource that thread A holds.

  We also wake a thread on the waiter list. We should wake the 

  Drawbacks:
    scheduler_donate_priority and scheduler_revoke_priority will require locating the thread in the list for its current priority level.
    This is somewhat expensive. Doing this without disabling interrupts seems awkward in a system with a high process count. 
    Doing it with interrupts enabled, though, makes it possible for the current holder of the lock to change out from under us.
    If we use a stale value for holder, we risk raising the priority of the holder PERMANENTLY, with nasty repercussions for 
    threads that actually need high priority.

    (because the old holder has released the lock already, and we donate to him assuming he will yield his priority later
      after releasing the lock)

  Q: Does this design handle nested donation?
  A: Yes.

  Promotion:
    L runs, acquires resource 1, and then yields.
    M runs, acquires resource 2, then tries to acquire resource 1. It blocks waiting for L. L's priority is raised to M.
    H runs, acquires resource 3, then tries to acquire resource 2. It blocks waiting for M. M's priority is raised to H.
      We then recurse on the thread for which M is waiting (if there is one). L's priority is raised to H.
      Promotion has been propagated appropriately.

  Demotion:
    Now L is scheduled. L releases the lock and no longer cares about any waiters on that lock. L examines the priorities the waiters on the resources it still holds,
      and modifies its priority to max( L's original priority, highest priority among waiters ).
      L can assert here that it finds no priority larger than the one it is giving up, 
      since a thread adding itself to waiters and then calling scheduler_donate_priority is done atomically.

    Proposal:
      Threads track the holder of the resource for which they are waiting, updating it before thread_block in the synchronization code.
      scheduler_donate_priority can work recursively on this field.

        ADVANCED SCHEDULER
        ==================

Design:

Quoting from the design doc: http://courses.cs.vt.edu/~cs5204/fall15-gback/pintos/doc/pintos_7.html#SEC131
    Multiple facets of the scheduler require data to be updated after a certain number of timer ticks. In every case, these updates should occur before any ordinary kernel thread has a chance to run, so that there is no chance that a kernel thread could see a newly increased timer_ticks() value but old scheduler data values. 

    Thread priority is calculated initially at thread initialization. It is also recalculated once every fourth clock tick, for every thread.

The only way to ensure that no kernel thread can see an increased timer_ticks() value but no 
scheduler data value is if the ticks and the updates are done atomically. The timer interrupt handler
is a logical place to put this computation.

This (hopefully) results in a system more responsive and fair to its applications.
The cost is that every X ticks we have a O(N) (where N is the number of threads) operation that is
pure system overhead. 

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0
 4
 8
12
16
20
24
28
32
36

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

I implemented it with a thin layer of abstraction:
I defined a type 'fp' that it just an int32_t. This makes the cost
of my fixed-point arithmetic low (e.g. no need to allocate a struct that
tracks integral and fractional portions of a number), while still 
allowing me to change the implementation later if I need to.

I did not use macros because I find they tend to be "write only" -- hard
for someone else to understand later.

         SURVEY QUESTIONS
         ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

