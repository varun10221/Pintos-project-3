      +--------------------+
      |        CS 140      |
      | PROJECT 1: THREADS |
      |   DESIGN DOCUMENT  |
      +--------------------+
           
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Jamie Davis <davisjam@vt.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

No preliminary comments.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

No other sources were consulted.

           ALARM CLOCK
           ===========

============================
TODO REWRITE THE ALARM CLOCK STUFF
============================


---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

timer_sleep() appends the running thread to the sleeping_queue.
This update to the sleeping_queue is kept atomic through the use of a lock.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

The situation here is that a thread is running timer_sleep and a timer
interrupt occurs. 

CASE: The timer interrupt handler traverses and potentially removes elements from the sleeping_queue.
It does not hold a lock. I don't see a good way around this race condition besides disabling interrupts
in timer_sleep while we modify sleeping_queue.

CASE: The timer interrupt handler just Ups a semaphore. This (eventually) wakes waker_thread, who
waits for the lock on the sleeping_queue before modifying it. No race condition since we revert to
well-defined lock semantics.

HOWEVER: if the timer interrupt handler interrupts a call to timer_sleep while we are computing
when to wake us up, and it uses X ticks, then we potentially sleep X + ticks_to_sleep ticks.
If X >> ticks_to_sleep, this could be bad.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

For Monday 31 Aug:
Jamie's proposed design

  As provided, timer_sleep calls thread_yield until ticks <= timer_elapsed.
  thread_yield puts the caller at the end of the ready list, leaves its state as THREAD_READY, and then schedules a thread to run
    (could be the caller). This can result in busy looping, using up CPU cycles scheduling and unscheduling the sleeping thread
    until enough ticks have passed. For short sleeps this is OK, but for long sleeps this wastes a lot of CPU.

  It would be preferable if the sleeping thread were not considered for scheduling until its requested number of ticks have passed.
  There is no requirement that the thread be woken up after exactly x ticks (unless the system is otherwise idle).

  Proposal 1: Complete accuracy, at the expense (perhaps) of system responsiveness
  
  - Introduce a new list of sleeping threads in thread.c: static struct list sleeping_queue
  - Add a wake_me_at field to struct thread, used to track the minimum value of ticks in 'ticks' before which we can be woken (this could also be done with a struct tracking a thread and a wake_me_at field, but this seems like a needless use of memory)
  - When a thread calls timer_sleep, it adds itself to this list and yields
  - In the timer interrupt handler, traverse the sleeping_queue and add to ready_queue any threads list that have finished their sleep

  Problem: If the system has a lot of threads that call timer_sleep, traversing the sleeping_queue to find threads that should be woken can be expensive.
    The interrupt handler must run as fast as possible to improve system responsiveness. We don't want to spend ticks iterating over a list.

  Variation: keep the sleeping_queue sorted by soonest-to-wake, minimizing the time spent traversing it in the timer interrupt handler. This makes insertion more expensive, which in turn makes the timer interrupt handler take longer.
    
  Proposal 2: System responsiveness, at the expense of timer accuracy

  (The first three are the same as Proposal 1)
  - Introduce a new list of sleeping threads in thread.c: static struct list sleeping_queue
  - Add a wake_me_at field to struct thread, used to track the minimum value of ticks in 'ticks' before which we can be woken
  Differences:
  - Introduce another "system" thread (joining the elite club of idle_thread and initial_thread). Call it waker_thread.
     waker_thread is always on the ready list and when scheduled, it:
      - iterates over the sleeping_queue
      - adds any ready-to-wake threads to the ready_queue
      - yields

  Here we're still burning CPU cycles busy-waiting, but in a high-sleeper-count scenario only *one* thread (the waker_thread) is being scheduled to burn cycles instead of multiple threads. In a low-sleeper-count scenario this is roughly the same as what we have now anyway.

  Variation 1: Instead of introducing a waker_thread, have the idle_thread check the sleeper_list instead. It's not doing anything else anyway. However, this could starve sleepers. The idle_thread only runs when the ready_queue is empty, so if there are always ready threads, idle would never run and thus sleepers would never be woken. Failure!

  Variation 2: Have waker_thread wait on a semaphore (monitor?) and have the timer interrupt "Up" the semaphore. This does not require busy waiting. waker_thread is therefore only ever scheduled to run by the timer interrupt, instead of at the same rate as every other thread in the ready list.

  Variation 3: Just stick sleeping threads in the ready_queue, mark them as SLEEPING, and don't schedule them until enough time has passed. This has minimal complexity but makes the scheduling activity pricey! I think the list traversal could become unpleasantly expensive.

  Files to modify:
    - thread.h: thread_status
    - thread.h: struct thread


Design update during priority queue testing:
  Using a raw priority_queue for the sleeping_queue seems to be too expensive to scan
  (at least to pass the alarm-multiple test).

  This may be due to the waste of timer cycles running the waker_thread unnecessarily.
  At the moment we run it any time there is a sleeper and timer_interrupt goes off.

Proposal 1:
  To reduce waste in running waker, we choose to keep the sleeping_queue sorted by wake time.
  T1 comes before T2 if
    T1.wake_me_at < T2.wake_me_at
  This causes us to spend extra CPU cycles inserting, but makes it much cheaper to determine whether
  or not there is a sleeping thread to wake. 
    > Compute MIN(wake_me_at of the first thread in each list of the priority queue)

  If only one priority level is dominant, the use of a 64-list queue is a mistake, since we
  don't derive much benefit from splitting it up and we have to spend awhile iterating
  through the 64 lists each time.

  Competing with Proposal 2, can we track the next time that ANY thread needs to be woken? 
  This would allow timer_interrupt to decide very quickly whether or not there is anything 
    for waker to do.

  - Add an int64_t field to the priority_queue struct, for general use
  - Any time a thread calls push_sleeping_queue, we update the field
  - Any time waker actually runs, it knows there is someone to wake up. It wakes the first elements
    of each list and peeks at the next element of each (if there is someone). It then computes
    MIN(peek value) and updates the field for timer_interrupt. Since it is doing so
    atomically w.r.t. sleeping_queue (it holds a lock), we have the invariant "the next time to wake
    field of sleeping_queue will always be correct" 

  Note that we cannot track the priority queue to start in, since waker may not run in the same tick
    that timer_interrupt goes off. Consequently there may be other threads at a higher priority
    level that should be woken first, even if they were supposed to be woken at a later tick
    than the min in the sleeping_queue.

Proposal 2:
  We could revert to using a list rather than a priority_queue for the sleeping_queue.
  We would keep it sorted by both priority AND wake_time, giving precedence to wake_time.
  T1 comes before T2 if
    T1.wake_me_at < T2.wake_me_at
      or
    T1.wake_me_at == T2.wake_me_at && T1.effective_priority < T2.effective_priority
       
  This would make it extremely cheap to determine whether or not there was someone to wake.
    > Check the wake_me_at time of the first element in the queue
  However, insertion would become expensive if there is a high thread count and there are multiple
  priority levels with high thread counts.

  If only one priority level is dominant, the cost of insertion would be comparable to the 
  prioritized list.

Both proposals rely on the fact that a thread cannot change its wake_me_at time until it is woken up,
  since re-sorting a list would be expensive.

-------------------

Final alarm clock design:
  After experiencing "surprising" thread behavior due to the interplay of the scheduler, the waker thread, and other threads
  doing sleeping, and after consultation with Dr. Back, I decided to change the design to look like the original Proposal 1,
  with a few tweaks.

1. Put the waker logic directly into timer_interrupt. Signaling the waker thread puts us at the mercy of the 
OS scheduler, an undesirable situation given the critical nature of the alarm clock.
2. Keep the sleeping_queue, but make it directly available to timer.h (no longer static). 
3. Keep this sleeping_queue a priority queue (for priority waking), and keep each list sorted by wake time.
  This allows us to quickly find and wake the relevant threads.
  
       PRIORITY SCHEDULING
       ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread:
    priority base_priority;             /* Base priority. */
    priority effective_priority;        /* Effective priority. */
    struct priority_queue_elem elem;    /* Priority queue element. */
    /* Tracks locks held by this thread. */
    struct list lock_list; /* Locks we hold. */
    /* Tracks the lock for which this thread is waiting, enabling
       nested priority donation. */
    struct lock *pending_lock; /* Lock we want. */

struct semaphore_elem:
    This new field allows us to identify the "largest" element
    in a list of semaphore_elem's. It allows priority wakeup.
    int64_t *sort_val;                   /* Value on which to sort. */

New files:
  priority_queue.h, priority_queue.c

  Define interface to and implementation of data structure 'struct priority_queue'

/* Mask what a priority is. */
typedef int64_t priority;

/* Like a list_elem, but for a priority queue. */
struct priority_queue_elem
  {
    struct list_elem elem; /* List element. */
    priority *p;           /* Priority of the element. */
    int64_t *sort_val;     /* Sort by this value if desired. */
  };

/* Priority queue, an ADT modeled after the list implementation. */
#define PRI_QUEUE_NLISTS (1 + PRI_MAX - PRI_MIN)
struct priority_queue
  {
    struct list queue[PRI_QUEUE_NLISTS];
    size_t size; /* Total number of elements in all lists. */
    /* Useful for the sleeping_queue. Could be handy in other places.
       In the sleeping list we track the minimum wake_me_at value. */  
    int64_t val; 
  };

These two were changed to be 'struct priority_queues' rather than 'struct lists'. 
This allows faster look-up time in the event of thousands or more threads.
I changed the names, too. They are declared as static structs invisible outside
of thread.c, so there is no risk of breaking existing applications.

  static struct priority_queue ready_queue;
  static struct priority_queue sleeping_queue;

synch.h:
  The waiters lists of condition variables will remain unsorted lists.
  The list elements (priority_queue_elem or semaphore_elem) both have
  a sort_val which is set to the address of the effective priority
  of the relevant thread, allowing us to determine the
  highest priority waiter just-in-time.

  Using lists rather than priority queues keeps the size of a 'struct lock' relatively small.
  Applications (e.g. priority-donate-chain.c) might declare enough locks
  to clobber the stack if priority queues were used instead of lists 
  in this setting. I started out using priority queues, and switched back
  when I ran priority-donate-chain and encountered stack corruption.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

TODO

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

The waiters list for each synchronization primitive has elements that
know their priority. With appropriately defined list_less_func's
(one for sema's and one for cond's), list_max can be used to find
the highest-priority waiter.

This takes O(N) time but we expect that synchronization primitives
will not have a large number of waiters at any given time.

For the sleeping_queue and the ready_queue, priority_queue_pop_front
finds the highest-priority waiter in O(1) time. These lists are
expected to be larger, and consequently O(1) time is a design requirement.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation. How is nested donation handled?

A thread determines that the lock is already held.
It determines whether or not the holder would accept a donation of
priority (i.e. does the thread have higher priority than the holder?).

If the holder would accept the donation:
  Since it is not running, the holder must be in either "ready" or "blocked" state.
    In "ready" state it is on the ready_queue.
    In "blocked" state it is on the sleeping_queue (if sleeping)
      or on the list of waiters associated with a synchronization primitive
      (sema, lock, or cond).
  The thread determines the state and location of the holder and promotes
    him (raises his effective priority)
    If on the ready queue or the sleeping queue, the holder must be relocated within
    the queue.
    If on the waiting list of a synchronization primitive, just changing the 
    effective priority is sufficient: the highest priority waiter is calculated "just in time" 
    in the synchronization primitives.

  If the holder is blocked waiting on a lock (as determined by the 
    'struct lock *lock_we_want' field in a 'struct thread'), the caller
    looks up the holder and repeats the operation up to MAX_DONATION_DEPTH times. 

  If the holder is blocked on a synchronization primitive other than a lock, there is no
    particular thread responsible for signaling the primitive. In this case 
    donation ceases, as there is no one to donate priority to.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

A thread releases a lock. If there are waiters, the thread
wakes the highest priority one using list_max on the list of waiters.

The thread holding the lock might have had priority donated to it.
Due to priority donation, we know that a thread cannot have a lower
priority than the thread it just woke. If the thread it woke has an equal 
priority, it may have had its current priority donated to it DUE TO OWNERSHIP
OF THIS LOCK. However, if it holds multiple locks then its current priority
could be due to a thread waiting on another lock that it holds.
Consequently, the thread checks through the locks it holds for the highest
waiter on each. 
  If it finds no waiter with its own priority, it knows that
the priority donated to it came from the thread it just woke.
The thread releasing the lock then "returns" its priority by reducing its effective priority
to the max of (its base priority, highest of those threads waiting for the locks it holds).

If a thread reduces its priority as a result of this calculation, the thread
releasing the lock yields to allow the higher-priority thread to run.

This gives precedence to the running thread in the event that it holds multiple locks
and that two or more locks are tied for "highest priority waiter".

Suppose threads A, B, and C, and locks L1, L2.
Thread    priority
  A          1
  B, C       2
A locks L1 and L2.
B locks L1, and later on C locks L2.
B and C both offer priority to A.

Now A runs, releases L1, and sees that B (the thread it just woke) also has priority 2.
A may or may not need to yield to him. A runs thread_return_priority and sees that C is 
waiting for L2 with priority 2, so A retains its "promotion" and does not yield.

We justify this by noting that if the running thread holds two locks, it might be holding more.
As a general rule, we would like it to release more locks to give other threads a chance to run.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

Suppose thread A (priority 2) holds a lock.
Thread B (priority 2) wants the lock.
Suppose thread_set_priority is a 2-step operation: 
  - change the base priority 
  - update effective priority based on the locks I hold

Suppose thread_donate_priority checks the effective priority but not the
  base priority.

Suppose thread_set_priority is not done atomically, so that a thread can
  be interrupted in between changing the base and changing the effective
  priority.

Thread A calls thread_set_priority(1), lowers its base priority, and is then interrupted.
  (base 1 effective 2).

Thread B calls lock_acquire, sees that thread A holds it, and "offers" its priority
to Thread A. Thread B sees that thread A has effective priority 2 and so does not
modify Thread A's effective priority. Thread B blocks and the scheduler runs.

*The scheduler runs and a thread with priority 1 (not A) is run.
  This is in defiance of priority scheduling: after Thread B blocks,
  Thread A should have priority 2 and should have been scheduled.

Thread A is now scheduled and updates its effective priority to 2 because B
is waiting for it.

The role of the scheduler in this race condition means that the only 
useful form of synchronization is to disable interrupts during thread_set_priority.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

As noted above, I considered replacing the waiters lists of synchronization
primitives with priority_queues. 

Changing a thread's priority during priority donation might require moving it within
its list. This operation would be trivial when using a priority queue.
However, a priority_queue is too large and can clobber the stack.
In addition, I consider it unlikely that the list of waiters for a given lock
would ever grow long enough for element relocation to have a meaningful performance impact.

Replacing the ready_queue and sleeping_queue with priority queues, however, seemed
like a logical step. These structures can reasonably be expected to hold thousands of elements.

Changing an element's priority in a 64-way array of lists consists of a list_remove operation (changes a few pointers)
followed by a list_push_back (changes a few more pointers).
To do so in a single list would be a O(n) operation.

        ADVANCED SCHEDULER
        ==================

Design:

Quoting from the design doc: http://courses.cs.vt.edu/~cs5204/fall15-gback/pintos/doc/pintos_7.html#SEC131
    Multiple facets of the scheduler require data to be updated after a certain number of timer ticks. In every case, these updates should occur before any ordinary kernel thread has a chance to run, so that there is no chance that a kernel thread could see a newly increased timer_ticks() value but old scheduler data values. 

    Thread priority is calculated initially at thread initialization. It is also recalculated once every fourth clock tick, for every thread.

The only way to ensure that no kernel thread can see an increased timer_ticks() value but no 
scheduler data value is if the ticks and the updates are done atomically. The timer interrupt handler
is a logical place to put this computation.

This (hopefully) results in a system more responsive and fair to its applications.
The cost is that every X ticks we have a O(N) (where N is the number of threads) operation that is
pure system overhead. 

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0
 4
 8
12
16
20
24
28
32
36

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

I implemented it with a thin layer of abstraction:
I defined a type 'fp' that it just an int32_t. This makes the cost
of my fixed-point arithmetic low (e.g. no need to allocate a struct that
tracks integral and fractional portions of a number), while still 
allowing me to change the implementation later if I need to.

I did not use macros because I find they tend to be "write only" -- hard
for someone else to understand later.

         SURVEY QUESTIONS
         ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

