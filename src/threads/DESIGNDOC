     +--------------------+
     |        CS 5204     |
     | PROJECT 1: THREADS |
     |   DESIGN DOCUMENT  |
     +--------------------+
          
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Jamie Davis <davisjam@vt.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

I consulted "the Internet" (stackoverflow, RMS's gdb guide, etc.) 
to learn how to use cscope and gdb.

Related to the actual content of my code, I did not consult 
any sources other than those named above.

While answering the linux questions, I referred to the following
high-level descriptions.

L1:
  http://www.ibm.com/developerworks/library/l-timers-list/
L2:
  http://www.linuxjournal.com/magazine/real-time-linux-kernel-scheduler?
L3:
  https://en.wikipedia.org/wiki/Completely_Fair_Scheduler
  linux/Documentation/scheduler/sched-design-CFS.txt

          ALARM CLOCK
          ===========

---- DATA STRUCTURES ----

>> A1. Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

lib/kernel/priority_queue.h:

/* A priority_queue is an abstract data type consisting of an array of lists,
optionally sorted. */

  struct priority_queue_elem
    {
      struct list_elem elem; /* List element. */
      priority *p;           /* Priority of the element. */
      int64_t *sort_val;     /* Sort by this value if desired. */
    };

#define PRI_QUEUE_NLISTS (1 + PRI_MAX - PRI_MIN)
  struct priority_queue
    {
      struct list queue[PRI_QUEUE_NLISTS];
      size_t size; /* Total number of elements in all lists. */
      /* Useful for the sleeping_queue. Could be handy in other places.
         In the sleeping list we track the minimum wake_me_at value. */  
      int64_t val; 
    };

thread.h:
  struct thread:
    /* Wake a thread in thread_status THREAD_BLOCKED when 0 < wake_me_at <= current timer tick */
    int64_t wake_me_at; 
    struct priority_queue_elem elem;    /* Priority queue element. */
                                           (used to be a struct list_elem elem)

thread.c
  /* Priority queue of sleeping processes. Each priority list is sorted by wake_me_at, soonest to latest. */
  static struct priority_queue sleeping_queue;

Implementation details: 
devices/timer.c::timer_sleep: 
  threads add themselves to sleeping_queue
devices/timer.c::timer_interrupt: 
  interrupt handler calls wake_sleeping_threads,
  which checks if the soonest-to-wake waiter in each list should be woken.
  sleeping_queue.val tracks minimum wake_me_at value across the entire list,
  which lets us quickly decide whether or not to scan the 64 lists in the priority queue.
  By starting at the high-priority end of the queue, we wake a higher-priority thread
  first.

---- COMPARISON TO LINUX ----

>> L1. What data structure does Linux's 4.2 use to implement 
>> its timer facility?  See https://github.com/torvalds/linux/blob/master/kernel/time/timer.c

Via schedule_timeout -> __mod_timer -> internal_add_timer,
sleeping threads are added to a list of sleepers, binned
so that "soon" threads are in the first bin, and later threads
are in subsequent bins covering (increasingly large?) ranges.

The function __next_timer_interrupt is used to identify the 
next-soonest timer that will go off.

When a sleeper's time to wake up occurs, a timer interrupt
is triggered (?) and moves the thread back into a ready state.
It looks like this can also be done via __run_timers?

The function cascade re-locates each timer in the appropriate
bin of a 'struct tvec_base'.

Comparison:
  Bucketing by wakeup time allows linux to add new timers more quickly
than my solution, which has to traverse an entire list (the priority list
in the PQ) looking for the right place to go.
Buckets reduce the potential traversal length.
  When waking threads, the two solutions are comparable, since both maintain 
sorted lists.

      PRIORITY SCHEDULING
      ===================

---- DATA STRUCTURES ----

>> B1. Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

thread.h:
  struct thread:
      priority base_priority;             /* Base priority. */
      priority effective_priority;        /* Effective priority. */
      struct priority_queue_elem elem;    /* Priority queue element. */
      /* Tracks locks held by this thread. 
         Allows us to determine the appropriate (potentially lower) priority level to set
         after releasing a lock, for "priority return." */
      struct list lock_list; 
      /* Tracks the lock for which this thread is waiting, enabling
         nested priority donation. */
      struct lock *pending_lock; /* Lock we want. */

synch.c:
  struct semaphore_elem:
    /* This new field allows us to identify the "largest" element
    in a list of semaphore_elem's. It allows priority wakeup for
    condition variables. It is set to &thr->effective_priority. */

    int64_t *sort_val;                   /* Value on which to sort. */

priority_queue.h, priority_queue.c:
  These files define the interface to and implementation of data structure 'struct priority_queue',
  described above in the alarm clock section.

  /* Mask what a priority is. */
  typedef int64_t priority;

thread.c:
  The ready_list declaration was changed to: 
    static struct priority_queue ready_queue;

Implementation details:
  We maintain sleeping_queue and ready_queue as 64-way priority lists,
  with equal-priority threads scheduled in FIFO fashion.
  Any changes to the priority of a thread in 
  either the ready queue or the sleeping queue requires changing the priority list 
  in which the thread resides.

  In synch.h, the waiters lists of condition variables remain unsorted lists.
  The list elements (priority_queue_elem or semaphore_elem) both have
  a sort_val which is set to the address of the effective priority
  of the relevant thread, allowing us to determine the
  highest priority waiter just-in-time as long as the effective_priority
  of the affected thread(s) is set properly.

---- COMPARISON TO LINUX ----

>> L2. Compare your ready queue design to Linux's design for how it maintains
>> its "real-time" tasks.
>> Look at pick_next_rt_entity in https://github.com/torvalds/linux/blob/master/kernel/sched/rt.c
>> How does your design differ?

In my design, there is a "queue" consisting of 64 lists, one for each possible priority level.
I do not differentiate between "real-time" tasks and "normal" tasks.

Linux has 140 possible priority levels, with the 40 highest levels only available to kernel
threads. Real time priorities range from 0 to 99.

In pick_next_rt_entity, the highest-priority entry in the 'struct rq' (run-queue?) is selected.
A 'struct rq' includes a 'struct rt_prio_array' which tracks MAX_RT_PRIO (i.e. 100) possible
priority levels.

The design for selection of a task is therefore largely the same:
Linux and my designs both involve an array of lists, one for each priority level.

Some differences:
  Linux offers a wider variety of priority levels.

  Linux uses a bitmap to more quickly locate the highest-priority non-empty list from
  which to extract an element, which allows it to avoid iterating over each list
  checking it. I did not use a bitmap so I have to iterate.

  The Linux design gives some weight to *where* to schedule the process (which CPU).
  There seems to be an rq for each CPU, so that tasks can be pushed to and from each other's CPU
    to fill idle cycles. 
  As Pintos is a uni-processor design I had no need to consider this.

  Linux uses locks to protect its rq information, rather than disabling interrupts
  like I do.

  Linux priorities are "inverted": 0 is highest priority, and 139 is lowest priority.
  In my pintos, larger priority values are stronger, up to PRI_MAX of 63.

With respect to the maintenance of a 'struct rq'...
  enqueue_pushable_task: Whenever a new, higher-priority task is pushed to an rq,
    the rt_rq.highest_prio.next value is updated.
  dequeue_pushable_task: Whenever a task is removed from the rq, the highest_prio.next field
    is updated.
  __enqueue_rt_entity: Add this element to the list of appropriate priority.
    Set the appropriate bit in the rq's bitmap to indicate the presence of a task at
    this priority.
  __dequeue_rt_entity: Delete this element from its rq. If it is the last element
    in the list, clear the appropriate bit in the rq's bitmap.

As far as I can tell, the designs are fundamentally the same,
except that Linux uses a bitmap for performance purposes and has to worry about
multiple CPUs.

       ADVANCED SCHEDULER
       ==================

---- DATA STRUCTURES ----

>> C1. Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

lib/kernel/fpra.c, fpra.h:
  Implementation of fixed-point real arithmetic with 14 fractional bits.

  /* Floating-point number. */
  typedef int32_t fp;
  
  /* fp's are divided into an integral and a fractional portion. 
     We allot 1 bit for sign, POINT bits for fractional, and the rest of the 
     bits (32 - 1 - POINT) to integral. */
  #define POINT 14
  
  enum round_mode {
    ROUND_TO_ZERO,
    ROUND_TO_NEAREST
  };
  
  static int ONE = 1 << POINT;

thread.h:
  struct thread:
    int nice; /* The 'nice' value in the mlfqs. */
    fp recent_cpu; /* The 'recent cpu' value in the mlfqs. */
    bool in_time_slice_list;     /* Whether or not we are in the time_slice_threads list. */
    struct list_elem ts_elem;    /* List element for time_slice_threads list. */

thread.c:
  /* List of threads that have run in the last TIME_SLICE ticks. 
     May contain duplicates. List will never be very large so this is OK. */
  static struct list time_slice_threads;

Implementation notes:
  Each time thread_tick() runs, a thread increments its recent_cpu by 1 and
  adds itself to time_slice_threads if(!thr->in_time_slice_list).

  Every 4 timer_tick()'s, we update the recent_cpu of the threads that have run since
  the last time timer_tick() went off (i.e. those in time_slice_threads).
  We then empty that list.
  Every TIMER_FREQ timer_ticks()'s, we recalculate load_avg and update the recent_cpu of all
  threads.

  Although TIME_SLICE == 4, we can't just update recent_cpu of the currently active thread.
  That thread may not have been the only still-alive thread running in this time slice. Another thread
  might have blocked or called thread_yield(), and the currently active thread have been scheduled in
  its place.

---- COMPARISON TO LINUX ----

>> L3. Compare your ready queue design to Linux's design for how it maintains
>> its ready queue for general purpose tasks.
>> See __enqueue_entity in https://github.com/torvalds/linux/blob/master/kernel/sched/fair.c

My "advanced scheduler" (4.4 BSD) ready queue design updates the CPU usage of running threads
every timer tick, and of all threads once per second (100 ticks).

A thread's priority is calculated based on its recent CPU usage, the system load (how many
different threads have been competing for the CPU recently), and how "nice" the thread is.
The more CPU a thread uses, and the "nicer" it is, the lower its priority.

Threads are stored in a "ready queue" (an array of 64 linked lists, one per priority).
Threads are run in a FIFO fashion, starting with the highest-priority list.

When a timer interrupt occurs, the running thread yields and is placed at the end of the
priority list to which it belongs, so that the CPU is shared relatively equally by threads
in that priority list.

Linux's "Completely Fair Scheduler" does not use a priority mechanism. Instead it uses
a time-ordered red-black tree, with the "key" being the amount of time that the thread
has consumed. New processes tend to fall on the left side of the tree; older processes that have run
a lot tend to get shifted to the right side of the tree.

A thread only yields the CPU if there is a task with a smaller total runtime, but Linux allows
some overlap to make sure that it doesn't over-schedule dueling tasks and inconvenience both of
them by eliminating the benefits of caching.

This brings us to one difference between the 4.4 BSD design I implemented and Linux's design.
In my implementation of the 4.4 BSD design, there are 64 priority lists. If there are multiple threads
at the maximum priority level (not unlikely given that the floating-point priority computation
is rounded to the nearest integer), they will always alternate turns every TIMER_SLICE ticks
due to the FIFO nature of the priority lists.
  In Linux, the running thread will only yield when it has consumed as much CPU time as the other thread,
plus a little bit. This means that in a simple 2-thread system, Linux's CFS scheduler will
allow threads to benefit from any caching the machine performs on their behalf. 

One benefit of using a list for a ready queue vs. an rb tree is that it is trivial to identify the next element
to run: it is at the front of the list. The Linux cfs_rq struct tracks the leftmost entry so that it can locate 
the next task to run in O(1) instead of O(log n).

Another difference is the degree of granularity. My recent_cpu calculation is fuzzier than linux's,
which tracks an exact runtime rather than a weighted moving average.

>> L4. How many bits does Linux's fixed-point arithmetic (used to compute loadavg) 
>> use for each fixed-point's fractional part?
>> Read: https://github.com/torvalds/linux/blob/master/include/linux/sched.h

11 bits.

From linux/sched.h:
  #define FSHIFT    11    /* nr of bits of precision */
  #define FIXED_1   (1<<FSHIFT) /* 1.0 as fixed-point */

        SURVEY QUESTIONS
        ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

I enjoyed this assignment. None of the parts were too difficult, although
a hint or two might have been helpful (e.g. don't write a separate
kernel thread for waking, or "consider carefully how to do priority donation
for sema's, since the list there is composed of sema_entry's rather than
(as the comment states) threads...").

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

I initially implemented the alarm clock with a semaphore Up'd by
timer_interrupt and Down'd by a kernel "waker" thread.
I got it working, but discovered that I couldn't pass all of the alarm
tests because the waker thread was subject to the idiosyncrasies of the
scheduler. I had a mostly-working alarm clock that would wake
threads "within a few ticks" of when they requested.

You suggested that some costs are worth paying in the timer interrupt handler,
and I rewrote with a somewhat-optimized priority queue that tracked
the minimum time-to-wake of all of the threads in the queue.

Overall this experience helped me understand the tradeoffs and design decisions
faced by OS developers, and really helped me see the amount of interplay between
moving parts -- the bad behavior between the timer interrupt handler, a kernel
thread, and the scheduler came as a surprise to me.

In fact, this exercise might be a useful assignment for future years: "When is it
OK to put computation into the timer interrupt handler, and when can you safely
offload it to another thread?".

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

N/A

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

N/A

>> Any other comments?

1. I appreciated that you provided the formulas for fixed-point real arithmetic.
That saved me a lot of time.

2. I think offering a 10-20 minute demo in using gdb and cscope to debug a problem might be helpful.
Those tools are really powerful and if people think "Nah, too much work to learn a new
tool" then they will not reap the benefits thereof.

3. Adding a "follow-up" component where groups review each other's designs or code might be useful.
Getting experience doing code reviews and design reviews in this complex area would be helpful,
and I would like to see what solutions other groups came up with.
